{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from collections import namedtuple\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from IPython.display import display, HTML\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso, LogisticRegression\n",
    "from sklearn.svm import SVR,LinearSVR,NuSVR\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor,RandomForestRegressor,ExtraTreesRegressor,AdaBoostRegressor\n",
    "from sklearn.feature_selection import RFECV\n",
    "from sklearn.metrics import r2_score,make_scorer\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import (RBF, Matern, RationalQuadratic,\n",
    "                                              ExpSineSquared, DotProduct,\n",
    "                                              ConstantKernel)\n",
    "\n",
    "\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter ,OrderedDict\n",
    "from itertools import compress\n",
    "# TODO: Import 'make_scorer', 'DecisionTreeRegressor', and 'GridSearchCV'\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from pandas.plotting import scatter_matrix\n",
    "# Import supplementary visualizations code visuals.py\n",
    "import visuals as vs\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Dataset=namedtuple('Dataset','exchange df')\n",
    "DatasetMLModel= namedtuple('DatasetMLModel','exchange  train_size tscv_split test_size X_train y_train X_test y_test scaler_features scaler_target')\n",
    "Regressor= namedtuple('Regressor','name regressor_class params type')\n",
    "FeatureSelection= namedtuple('FeatureSelection','dataset regressor params RFECV')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('datasets/log_divided_close_datasets.pkl', 'rb') as input4:\n",
    "    log_divided_close_datasets = pickle.load(input4)   \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "features=['volume','amount', 'avg_price','open','high','quantity',\n",
    "          'EWMA26','EWMA12','EWMA9','log_return',\n",
    "          'Bollinger Upper', 'Bollinger Lower','Heiking_Close','Heiking High',\n",
    "          'Heiking Open','log_MACD','Variance12']\n",
    "def clean_ouliers(dataset,features):\n",
    "    \n",
    "    outliers_list=[]\n",
    "    for feature in features:\n",
    "\n",
    "        # TODO: Calculate Q1 (25th percentile of the data) for the given feature\n",
    "        Q1 = np.percentile(dataset.df[feature],25)\n",
    "\n",
    "        # TODO: Calculate Q3 (75th percentile of the data) for the given feature\n",
    "        Q3 = np.percentile(dataset.df[feature],75)\n",
    "\n",
    "        # TODO: Use the interquartile range to calculate an outlier step (1.5 times the interquartile range)\n",
    "        step = (Q3-Q1)*1.5\n",
    "        df_outlier_perfeature=dataset.df[~((dataset.df[feature] >= Q1 - step) & \n",
    "                                         (dataset.df[feature] <= Q3 + step))][[feature]]\n",
    "        df_outlier_perfeature[feature]=1\n",
    "        # Display the outliers\n",
    "        # print (\"Data points considered outliers for the feature '{}':\".format(feature))\n",
    "        # display(dataset.df[~((dataset.df[feature] >= Q1 - step) & (dataset.df[feature] <= Q3 + step))][feature])\n",
    "        outliers_list.append(df_outlier_perfeature)\n",
    "        \n",
    "    outliers=pd.concat(outliers_list,axis=1)\n",
    "    outliers=outliers.fillna(0)\n",
    "    outliers['count']=outliers.sum(axis=1)\n",
    "    return outliers\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outliers_index.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_the_last_row(dataset):\n",
    "    dataset.df.drop(dataset.df.index[-1], inplace=True)\n",
    "    return dataset\n",
    "def making_targets(dataset):\n",
    "    column_targets=['Returns','close','log_return']\n",
    "    for column_target in column_targets:\n",
    "        dataset.df[column_target+'_Target']=dataset.df[column_target].shift(-1)\n",
    "    return drop_the_last_row(dataset)\n",
    "\n",
    "def building_series_to_each_input(dataset,N=15,column_series=['close']):  \n",
    "    for n in range(1,N+1):\n",
    "        for column in column_series:\n",
    "            dataset.df[column+'_{0:02d}'.format(n)]=dataset.df[column].shift(n)\n",
    "    \n",
    "    dataset.df.drop(dataset.df.index[:n], inplace=True)\n",
    "    return dataset\n",
    "\n",
    "def building_series_to_each_input_log_return(dataset,N=15):\n",
    "    return building_series_to_each_input(column_series=['log_return'])\n",
    "\n",
    "def building_series_to_each_input_log_return(dataset,N=15):\n",
    "    return building_series_to_each_input(column_series=['Returns'])\n",
    "\n",
    "def make_X_Y(dataset , train_size,features=['close'],target=['close_Target']):    \n",
    "    X_train=dataset.df[features][:train_size]\n",
    "    y_train=dataset.df[target][:train_size]\n",
    "    X_test=dataset.df[features][train_size:]\n",
    "    y_test=dataset.df[target][train_size:]\n",
    "    return X_train,y_train,X_test,y_test\n",
    "\n",
    "def scaler(X_train,y_train,X_test,y_test):   \n",
    "    scaler_features = StandardScaler()\n",
    "    scaler_target = StandardScaler()\n",
    "    scaled_X_train=scaler_features.fit_transform(X_train)\n",
    "    scaled_X_test=scaler_features.transform(X_test)\n",
    "    X_train = pd.DataFrame(scaled_X_train, index=X_train.index, columns=X_train.columns)\n",
    "    X_test = pd.DataFrame(scaled_X_test, index=X_test.index, columns=X_test.columns)\n",
    "    scaled_y_train=scaler_target.fit_transform(y_train)\n",
    "    scaled_y_test=scaler_target.transform(y_test)\n",
    "    y_train = pd.DataFrame(scaled_y_train, index=y_train.index, columns=y_train.columns)\n",
    "    y_test = pd.DataFrame(scaled_y_test, index=y_test.index, columns=y_test.columns)\n",
    "    \n",
    "    return X_train,y_train,X_test,y_test, scaler_features,scaler_target\n",
    "\n",
    "\n",
    "def preprossessing(dataset,features,target, train_percent=0.80,evaluation_percent=0.20  ):\n",
    "    rows, columns=dataset.df.shape\n",
    "    train_size= int(rows*train_percent)\n",
    "    tscv_split=int(rows//(train_size*evaluation_percent)-1)\n",
    "    test_size=rows-train_size\n",
    "    X_train,y_train,X_test,y_test=make_X_Y(dataset,train_size,features,target)\n",
    "    outliers=clean_ouliers(X_train,features)\n",
    "    outliers_index=outliers[outliers['count']>=7].sort_values('count',ascending=False).index.values\n",
    "    X_train.drop(outliers_index, inplace=True)\n",
    "    X_train,y_train,X_test,y_test, scaler_features,scaler_target=scaler( X_train,y_train,X_test,y_test)\n",
    "    return DatasetMLModel(dataset.exchange,train_size ,tscv_split,test_size,\n",
    "                          X_train,y_train,X_test,y_test,scaler_features=None,scaler_target=None)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "features=['volume','amount', 'avg_price','open','high','quantity',\n",
    "          'EWMA26','EWMA12','EWMA9','log_return',\n",
    "          'Bollinger Upper', 'Bollinger Lower','Heiking_Close','Heiking High',\n",
    "          'Heiking Open','log_MACD','Variance12']\n",
    "target=['log_return_Target']\n",
    "\n",
    "#dataset.df[features][]\n",
    "dataset=making_targets(log_divided_close_datasets['btc_brl'])\n",
    "dataset= preprossessing(dataset,features,target)\n",
    "df_to_plot=pd.concat([dataset.X_train,dataset.y_train],axis=1)\n",
    "scatter_matrix(df_to_plot,alpha=0.1,figsize=(15,15), diagonal = 'kde')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns;\n",
    "ax = sns.heatmap(df_to_plot.corr(method='pearson'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "# TODO: Apply PCA by fitting the good data with only two dimensions\n",
    "pca = PCA(n_components=2,  whiten=True)\n",
    "pca.fit(dataset.X_train)\n",
    "# TODO: Transform the good data using the PCA fit above\n",
    "reduced_data = pca.transform(dataset.X_train)\n",
    "# Create a DataFrame for the reduced data\n",
    "reduced_data = pd.DataFrame(reduced_data, columns = ['Dimension 1', 'Dimension 2'])\n",
    "vs.biplot(dataset.X_train, reduced_data, pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# TODO: Apply PCA by fitting the good data with the same number of dimensions as features\n",
    "pca = PCA(n_components=4, whiten=True,iterated_power=7)\n",
    "# TODO: Transform log_samples using the PCA fit above\n",
    "pca_samples = pca.fit(dataset.X_train)\n",
    "\n",
    "# Generate PCA results plot\n",
    "pca_results = vs.pca_results(dataset.X_train, pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernels = [ConstantKernel(1.0, (1e-3, 1e3))*(DotProduct(sigma_0=5.0, sigma_0_bounds=(0.0, 10.0)) ** 2),\n",
    "           ConstantKernel(1.0, (1e-3, 1e3))*(DotProduct(sigma_0=10.0, sigma_0_bounds=(0.0, 10.0)) ** 2),\n",
    "           ConstantKernel(1.0, (1e-3, 1e3))*(DotProduct(sigma_0=1.0, sigma_0_bounds=(0.0, 10.0)) ** 2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters_per_regressor={\n",
    "    'NuSVR': {'C': [0.1],\n",
    "           'gamma': [0.09,0.1,0.2], \n",
    "           'kernel': ['rbf'],\n",
    "            'nu':[0.6],\n",
    "           'max_iter':[15000]},\n",
    "     'SVR': {'C': [0.1,0.01],\n",
    "           'gamma': [0.08,0.1,0.2],\n",
    "           'kernel': ['rbf'],\n",
    "           'degree': [2,3,4],\n",
    "           'max_iter':[15000]},    \n",
    "\n",
    "   'KernelRidge': {'alpha':range(2500,3500,100),\n",
    "                   'gamma': [1],\n",
    "                   'degree': [2,3],\n",
    "                   'kernel': [ 'poly']},\n",
    "    'GradientBoostingRegressor':{'loss' : [ 'huber'],\n",
    "                             'n_estimators':range(5,20,1),\n",
    "                             'alpha':[0.02,0.01,0.03],\n",
    "                             'max_depth':[2,3]\n",
    "                            },\n",
    "    'AdaBoostRegressor':{'base_estimator':[\n",
    "                                           NuSVR(C=0.1,nu=0.70),\n",
    "                                           NuSVR(C=0.1,nu=0.6),\n",
    "                                           NuSVR(C=0.1,nu=0.8)\n",
    "                                          ],\n",
    "                         'loss':['linear', 'square', 'exponential'],\n",
    "                      'n_estimators':range(10,300,10)},\n",
    "    'RandomForestRegressor':{'max_depth':[2,3],\n",
    "                         'n_estimators':range(70,90,2),\n",
    "                        'bootstrap':[True]},\n",
    "    'ExtraTreesRegressor':{'max_depth':[2,3],\n",
    "                         'n_estimators':range(180,300,10),\n",
    "                        'bootstrap':[True]},\n",
    "    'GaussianProcessRegressor':{'kernel':kernels} , \n",
    "    'LinearSVR': {'C': [0.4],\n",
    "           'loss': ['epsilon_insensitive'],\n",
    "           'max_iter':[15000]},\n",
    "    'LinearRegression':{},\n",
    "    'Lasso': { 'alpha':range(10,110,10)},\n",
    "    'DecisionTreeRegressor': {'max_depth':range(1,4),'criterion':['mse','friedman_mse']},\n",
    "}\n",
    "regressors={\n",
    "    'LinearRegression': Regressor('LinearRegression', LinearRegression, None,'linear_model'), \n",
    "    'Lasso':Regressor('Lasso', Lasso, None,'linear_model'),\n",
    "    'DecisionTreeRegressor':Regressor('DecisionTreeRegressor', DecisionTreeRegressor, None,'tree'),\n",
    "    'GradientBoostingRegressor':Regressor('GradientBoostingRegressor', GradientBoostingRegressor, None,'ensemble'),\n",
    "    'RandomForestRegressor':Regressor('RandomForestRegressor',RandomForestRegressor,None,'ensemble'),\n",
    "    'ExtraTreesRegressor':Regressor('ExtraTreesRegressor',ExtraTreesRegressor,None,'ensemble'),\n",
    "    'AdaBoostRegressor':Regressor('AdaBoostRegressor',AdaBoostRegressor,None,'ensemble'),\n",
    "    'SVR':Regressor('SVR', SVR, None,'svm'),\n",
    "    'NuSVR':Regressor('NuSVR', NuSVR, None,'svm'),\n",
    "    'LinearSVR':Regressor('LinearSVR', LinearSVR, None,'svm'),\n",
    "    'GaussianProcessRegressor':Regressor('GaussianProcessRegressor', SVR, None,'svm'),\n",
    "    'KernelRidge':Regressor('KernelRidge', KernelRidge, None,'svm')\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def performance_metric(y_true, y_predict):\n",
    "    \"\"\" Calculates and returns the performance score between \n",
    "        true and predicted values based on the metric chosen. \"\"\"\n",
    "    score = r2_score(y_true,y_predict)\n",
    "    return score\n",
    "def fit_all(regressors,parameters_per_regressor,dataset, pca):\n",
    "    columns=['Regressor','Score','BestEstimator']\n",
    "    data=[]\n",
    "    scoring_fnc = make_scorer(performance_metric)\n",
    "    for regressor_key, regressor  in regressors.items():\n",
    "        try :\n",
    "            print(regressor_key)\n",
    "            reg=regressor.regressor_class()\n",
    "            parameters=parameters_per_regressor[regressor_key]\n",
    "            cvts=TimeSeriesSplit(n_splits=dataset.tscv_split)\n",
    "            grid = GridSearchCV(reg, parameters,scoring_fnc, cv=cvts,error_score=-math.inf)   \n",
    "            grid = grid.fit(pca.transform(dataset.X_train), dataset.y_train.values.ravel())\n",
    "            data_row=[\n",
    "                regressor_key,\n",
    "                grid.best_score_,\n",
    "                grid.best_estimator_\n",
    "                ]\n",
    "            \n",
    "        except  Exception as exp:\n",
    "            print(exp)\n",
    "            data_row=[\n",
    "                regressor_key,\n",
    "                -math.inf,\n",
    "                None\n",
    "                ]\n",
    "        finally: \n",
    "            data.append(data_row)\n",
    "            \n",
    "    result_cv=pd.DataFrame(data,columns=columns)\n",
    "    result_cv.set_index(['Regressor'],inplace=True)\n",
    "    return result_cv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores=fit_all(regressors,parameters_per_regressor,dataset,pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores.sort_values('Score',ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores['BestEstimator']['AdaBoostRegressor']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.logspace(-3, 3, num=10, endpoint=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "int(10.869)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.logspace(np.log10(800),3,num=10,endpoint=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "types=(int,float,object,str)\n",
    "i=10.8697\n",
    "types[2]==str\n",
    "np.linspace(2, 2, num=10,endpoint=True).astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "kernels = [ConstantKernel(1.0, (1e-3, 1e3))*(DotProduct(sigma_0=5.0, sigma_0_bounds=(0.0, 10.0)) ** 2),\n",
    "           ConstantKernel(1.0, (1e-3, 1e3))*(DotProduct(sigma_0=10.0, sigma_0_bounds=(0.0, 10.0)) ** 2),\n",
    "           ConstantKernel(1.0, (1e-3, 1e3))*(DotProduct(sigma_0=1.0, sigma_0_bounds=(0.0, 10.0)) ** 2)]\n",
    "\n",
    "parameters_settings={\n",
    "    \n",
    "    'NuSVR': {'C':(np.logspace(-3,3,num=10,endpoint=True),float,np.logspace,-3,3,True),\n",
    "           'gamma':(np.logspace(-3,3,num=10,endpoint=True),float,np.logspace,-3,3,True), \n",
    "           'kernel':(['linear', 'poly', 'rbf', 'sigmoid'],object,None,None,None,False), \n",
    "           'nu':(np.linspace(0.1, 0.9, num=10,endpoint=True),float,np.linspace,0.1,0.9,True),\n",
    "           'max_iter':([15000],int,None,None,None,False)\n",
    "             },\n",
    "    \n",
    "    'SVR': {'C':(np.logspace(-3,3,num=10,endpoint=True),float,np.logspace,-3,3,True),\n",
    "           'gamma':(np.logspace(-3,3,num=10,endpoint=True),float,np.logspace,-3,3,True), \n",
    "           'kernel':(['linear', 'poly', 'rbf', 'sigmoid'],object,None,None,None,False), \n",
    "           'degree': (np.linspace(1, 10, num=10,endpoint=True),float,np.linspace,1,10,True),\n",
    "           'max_iter':([15000],int,None,None,None,False)\n",
    "           },\n",
    "    \n",
    "    'KernelRidge': {'alpha':(np.logspace(0,6,num=10,endpoint=True),float,np.logspace,0,6,True),\n",
    "                   'gamma':(np.logspace(-3,3,num=10,endpoint=True),float,np.logspace,-3,3,True), \n",
    "                   'degree': (np.linspace(1, 10, num=10,endpoint=True),float,np.linspace,1,10,True),  \n",
    "                   'kernel':(['linear', 'poly', 'rbf', 'sigmoid'],object,None,None,None,False)\n",
    "                   },\n",
    "    \n",
    "    'GradientBoostingRegressor':{'loss' : ([ 'ls', 'lad', 'huber', 'quantile'],object,None,None,None,False), \n",
    "                             'n_estimators':(np.logspace(0,3,num=10,endpoint=True),int,np.logspace,0,3,True),\n",
    "                             'alpha':(np.logspace(-3,-0.022,num=10,endpoint=True),float,np.logspace,-3,-0.022,True), \n",
    "                             'max_depth':(np.linspace(1, 10, num=10,endpoint=True),int,None,None,None,False)\n",
    "                            },                  \n",
    "                    \n",
    "    'AdaBoostRegressor':{'base_estimator':([\n",
    "                                           NuSVR(C=0.1,nu=0.25),\n",
    "                                           NuSVR(C=0.1,nu=0.50),\n",
    "                                           NuSVR(C=0.1,nu=0.75),\n",
    "                                           NuSVR(C=0.1,nu=0.9)\n",
    "                                          ],object,None,None,None,False),\n",
    "                         'loss':(['linear', 'square', 'exponential'],str,None,None,None,False), \n",
    "                      'n_estimators':(np.logspace(0,2.62,num=5,endpoint=True),int,np.logspace,0,3,True)  \n",
    "                        } ,\n",
    "    'RandomForestRegressor':{'max_depth':(np.linspace(1, 10, num=10,endpoint=True),int,np.linspace,1,10,True),\n",
    "                         'n_estimators':(np.logspace(0,3,num=10,endpoint=True),int,np.logspace,0,3,True) } ,\n",
    "                    \n",
    "    'ExtraTreesRegressor':{'max_depth':(np.linspace(1, 10, num=10,endpoint=True),int,np.linspace,1,10,True),\n",
    "                         'n_estimators':(np.logspace(0,3,num=10,endpoint=True),int,np.logspace,0,3,True) } ,\n",
    "    'GaussianProcessRegressor':{'kernel':(kernels,object,None,None,None,False)} , \n",
    "    'LinearSVR': {'C':(np.logspace(-3,3,num=10,endpoint=True),float,np.logspace,-3,3,True),\n",
    "           'loss':( ['epsilon_insensitive', 'squared_epsilon_insensitive' ],object,None,None,None,False),\n",
    "           'max_iter':([15000],int,None,None,None,False)},\n",
    "    'LinearRegression':{},\n",
    "    'Lasso': { },\n",
    "    'DecisionTreeRegressor': {}\n",
    "    }  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_parameters_per_ML(parameters_settings):\n",
    "    return {ML_algo:{parameter:settings[0] \n",
    "                     if type(settings[0])==object \n",
    "                     else  np.array(settings[0]).astype(settings[1])\n",
    "                     for parameter, settings in parameters.items()} \n",
    "            for ML_algo,parameters in  parameters_settings.items()}\n",
    "parameters_per_regressor=get_parameters_per_ML(parameters_settings)\n",
    "parameters_per_regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores=fit_all(regressors,parameters_per_regressor,dataset,pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def next_parameter_iteration(scores,parameters_settings,num=10,endpoint=True):\n",
    "    new_parameters_settings={}\n",
    "    for regressor, parameters in parameters_settings.items():\n",
    "        parameter_setting={}\n",
    "        if scores['BestEstimator'][regressor]:\n",
    "            params=scores['BestEstimator'][regressor].get_params() \n",
    "            \n",
    "            for parameter, value in parameters_settings[regressor].items():\n",
    "                options,tuplet_type,space_function,start,end,is_all_catagories=value\n",
    "                options=np.array(options).astype(tuplet_type)\n",
    "\n",
    "                print('*********')\n",
    "                print(regressor)\n",
    "                print(params[parameter])\n",
    "                print(options)\n",
    "              \n",
    "                if not is_all_catagories:\n",
    "                    options=[params[parameter]]\n",
    "                else:\n",
    "                    index,=np.where(options==params[parameter])\n",
    "                    index=np.asscalar(index[0])\n",
    "                    print(index)\n",
    "                    if index==0:\n",
    "                        options=[params[parameter]]\n",
    "                    elif index==len(options)-1:\n",
    "                        if space_function== np.linspace:\n",
    "                            end=int(math.log10(end))+1\n",
    "                            start=np.asscalar(options[index-1])\n",
    "                        if space_function== np.logspace:\n",
    "                            end=end+1\n",
    "                            start=end-1\n",
    "                    else :\n",
    "                        if space_function== np.linspace:\n",
    "                            end=np.asscalar(options[index+1])\n",
    "                            start=np.asscalar(options[index-1])\n",
    "\n",
    "                        if space_function== np.logspace:\n",
    "                            end=math.log10(options[index+1])\n",
    "                            start=math.log10(options[index-1])                 \n",
    "                \n",
    "                \n",
    "                \n",
    "                options=space_function(start,end,num,endpoint) if  space_function else options\n",
    "                parameter_setting[parameter]=(options,tuplet_type,space_function,start,end,is_all_catagories)\n",
    "            new_parameters_settings[regressor]=parameter_setting\n",
    "    return new_parameters_settings\n",
    "\n",
    "def tune_models (parameters_settings, iterations,dataset,pca):\n",
    "    for i in range(iterations):\n",
    "        parameters_per_regressor=get_parameters_per_ML(parameters_settings)\n",
    "        scores=fit_all(regressors,parameters_per_regressor,dataset,pca)\n",
    "        parameters_settings=next_parameter_iteration(scores,parameters_settings,num=10,endpoint=True)\n",
    "    return scores,parameters_settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "scores1=scores\n",
    "\n",
    "parameters_settings1=next_parameter_iteration(scores1,parameters_settings)\n",
    "\n",
    "scores4, parameters_settings4=tune_models(parameters_settings1, 4,dataset,pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores2.sort_values('Score',ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bool(None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
